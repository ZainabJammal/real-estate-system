import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import Ridge
from sklearn.ensemble import StackingRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import joblib
import warnings

# --- Setup ---
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
optuna.logging.set_verbosity(optuna.logging.WARNING)

def train_and_evaluate_model():
    """
    V9: Builds a Stacking Ensemble model, combining the strengths of a
    LightGBM model and a robust Ridge linear model.
    """
    # --- 1. Load and Clean Data ---
    print("Step 1: Loading and Cleaning Data...")
    df = pd.read_csv('properties.csv')
    df.drop_duplicates(subset=['id'], inplace=True)
    df = df[(df['price_$'] > 1000) & (df['size_m2'] > 10)].copy()

    # --- 2. Per-Type Outlier Handling ---
    print("\nStep 2: Handling Outliers Within Each Property Type...")
    def remove_outliers_by_group(data, group_col, target_col, lower_q=0.01, upper_q=0.99):
        q_low = data.groupby(group_col)[target_col].transform(lambda x: x.quantile(lower_q))
        q_high = data.groupby(group_col)[target_col].transform(lambda x: x.quantile(upper_q))
        return data[(data[target_col] >= q_low) & (data[target_col] <= q_high)]

    original_rows = len(df)
    df = remove_outliers_by_group(df, 'type', 'price_$')
    df = remove_outliers_by_group(df, 'type', 'size_m2')
    print(f"Removed {original_rows - len(df)} rows as per-type outliers.")

    # --- 3. Feature Engineering ---
    print("\nStep 3: Feature Engineering...")
    types_with_rooms = ['Apartment', 'House/Villa', 'Chalet', 'Office', 'Residential Building']
    df['bedrooms'] = np.where((df['type'].isin(types_with_rooms)) & (df['bedrooms'] == 0), np.nan, df['bedrooms'])
    df['bathrooms'] = np.where((df['type'].isin(types_with_rooms)) & (df['bathrooms'] == 0), np.nan, df['bathrooms'])
    for col in ['bedrooms', 'bathrooms']:
        df[col].fillna(df.groupby('type')[col].transform('median'), inplace=True)
        df[col].fillna(1, inplace=True)
        df[col] = df[col].astype(int)

    district_price_per_m2 = df.groupby('district')['price_$'].sum() / df.groupby('district')['size_m2'].sum()
    df['district_price_per_m2'] = df['district'].map(district_price_per_m2)
    df['district_price_per_m2'].fillna(df['district_price_per_m2'].median(), inplace=True)

    print("Creating geospatial features...")
    kmeans = KMeans(n_clusters=15, random_state=42, n_init='auto')
    df['location_cluster'] = kmeans.fit_predict(df[['latitude', 'longitude']]).astype(str)
    
    # --- 4. Data Splitting & Preprocessing Definition ---
    print("\nStep 4: Defining Preprocessing and Splitting Data...")
    X = df.drop(columns=['id', 'city', 'created_at', 'price_$'])
    y = np.log1p(df['price_$'])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    numerical_features = ['size_m2', 'latitude', 'longitude', 'district_price_per_m2']
    count_features = ['bedrooms', 'bathrooms']
    categorical_features = ['district', 'province', 'type', 'location_cluster']

    # We create a single, powerful preprocessor to feed into our models
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('counts', StandardScaler(), count_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='drop'
    )

    # --- 5. Define the Ensemble Model ---
    print("\nStep 5: Defining the Stacking Ensemble...")
    
    # Define the base models
    estimators = [
        ('lgbm', Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', lgb.LGBMRegressor(
                objective='regression_l1',
                random_state=42,
                n_estimators=1000,
                learning_rate=0.03, # Using solid, pre-tuned params
                num_leaves=40,
                max_depth=7,
                min_child_samples=20,
                colsample_bytree=0.8,
                subsample=0.8
            ))
        ])),
        ('ridge', Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', Ridge(alpha=1.0, random_state=42))
        ]))
    ]

    # Define the final meta-model that will combine the predictions
    final_estimator = Ridge(random_state=42)

    # Create the Stacking Regressor
    stacking_model = StackingRegressor(
        estimators=estimators,
        final_estimator=final_estimator,
        cv=5, # Use 5-fold cross-validation to train the meta-model
        n_jobs=-1
    )

    # --- 6. Train the Ensemble Model ---
    print("\nStep 6: Training the Stacking Ensemble Model...")
    print("(This will take a moment as it trains multiple models)")
    
    stacking_model.fit(X_train, y_train)
    
    print("Ensemble model training complete.")

    # --- 7. Final Evaluation ---
    print("\n--- Step 7: Final Model Evaluation on Test Set ---")
    y_pred_log = stacking_model.predict(X_test)
    y_pred_dollars = np.expm1(y_pred_log)
    y_test_dollars = np.expm1(y_test)

    r2 = r2_score(y_test_dollars, y_pred_dollars)
    mae = mean_absolute_error(y_test_dollars, y_pred_dollars)

    print(f"\nFinal R-squared (RÂ²): {r2:.4f}")
    print(f"Final Mean Absolute Error (MAE): ${mae:,.2f}")

    # --- 8. Save the Final Model and Supporting Objects ---
    model_filename = 'property_price_stacking_model_final.joblib'
    print(f"\nStep 8: Saving final model and supporting objects...")
    joblib.dump(stacking_model, model_filename)
    joblib.dump(kmeans, 'location_cluster_model.joblib')
    joblib.dump(district_price_per_m2, 'district_price_model.joblib')
    print(f"Models saved successfully to {model_filename} and supporting files.")

if __name__ == '__main__':
    train_and_evaluate_model()
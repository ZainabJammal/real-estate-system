import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
import joblib
import warnings

# --- Setup ---
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

def train_and_evaluate_model():
    """
    V10: A robust, simplified, and powerful approach using GridSearchCV.
    This architecture is guaranteed to avoid the NotFittedError while still
    performing advanced feature engineering and hyperparameter tuning.
    """
    # --- 1. Load and Clean Data ---
    print("Step 1: Loading and Cleaning Data...")
    df = pd.read_csv('properties.csv')
    df.drop_duplicates(subset=['id'], inplace=True)
    df = df[(df['price_$'] > 1000) & (df['size_m2'] > 10)].copy()

    # --- 2. Per-Type Outlier Handling ---
    print("\nStep 2: Handling Outliers Within Each Property Type...")
    def remove_outliers_by_group(data, group_col, target_col, lower_q=0.01, upper_q=0.99):
        q_low = data.groupby(group_col)[target_col].transform(lambda x: x.quantile(lower_q))
        q_high = data.groupby(group_col)[target_col].transform(lambda x: x.quantile(upper_q))
        return data[(data[target_col] >= q_low) & (data[target_col] <= q_high)]

    original_rows = len(df)
    df = remove_outliers_by_group(df, 'type', 'price_$')
    df = remove_outliers_by_group(df, 'type', 'size_m2')
    print(f"Removed {original_rows - len(df)} rows as per-type outliers.")

    # --- 3. Feature Engineering ---
    print("\nStep 3: Feature Engineering...")
    types_with_rooms = ['Apartment', 'House/Villa', 'Chalet', 'Office', 'Residential Building']
    df['bedrooms'] = np.where((df['type'].isin(types_with_rooms)) & (df['bedrooms'] == 0), np.nan, df['bedrooms'])
    df['bathrooms'] = np.where((df['type'].isin(types_with_rooms)) & (df['bathrooms'] == 0), np.nan, df['bathrooms'])
    for col in ['bedrooms', 'bathrooms']:
        df[col].fillna(df.groupby('type')[col].transform('median'), inplace=True)
        df[col].fillna(1, inplace=True)
        df[col] = df[col].astype(int)

    district_price_per_m2 = df.groupby('district')['price_$'].sum() / df.groupby('district')['size_m2'].sum()
    df['district_price_per_m2'] = df['district'].map(district_price_per_m2)
    df['district_price_per_m2'].fillna(df['district_price_per_m2'].median(), inplace=True)

    print("Creating geospatial features...")
    kmeans = KMeans(n_clusters=15, random_state=42, n_init='auto')
    df['location_cluster'] = kmeans.fit_predict(df[['latitude', 'longitude']]).astype(str)
    
    # --- 4. Data Splitting & Preprocessing Definition ---
    print("\nStep 4: Defining Preprocessing and Splitting Data...")
    X = df.drop(columns=['id', 'city', 'created_at', 'price_$'])
    y = np.log1p(df['price_$'])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    numerical_features = ['size_m2', 'latitude', 'longitude', 'district_price_per_m2']
    count_features = ['bedrooms', 'bathrooms']
    categorical_features = ['district', 'province', 'type', 'location_cluster']

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('counts', StandardScaler(), count_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='drop'
    )

    # --- 5. Define Model Pipeline and Hyperparameter Grid ---
    print("\nStep 5: Defining Model Pipeline and Hyperparameter Grid for GridSearchCV...")
    
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', lgb.LGBMRegressor(objective='regression_l1', random_state=42))
    ])
    
    # A focused grid for GridSearchCV to keep it fast but effective
    param_grid = {
        'regressor__n_estimators': [500, 800],
        'regressor__learning_rate': [0.03, 0.05],
        'regressor__num_leaves': [31, 40],
        'regressor__max_depth': [6, 8]
    }

    # --- 6. Run GridSearchCV ---
    print("\nStep 6: Running GridSearchCV...")
    grid_search = GridSearchCV(
        pipeline, param_grid, cv=5, 
        scoring='neg_mean_absolute_error', 
        n_jobs=-1, verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print("\nBest parameters found: ", grid_search.best_params_)
    best_model = grid_search.best_estimator_

    # --- 7. Final Evaluation ---
    print("\n--- Step 7: Final Model Evaluation on Test Set ---")
    y_pred_log = best_model.predict(X_test)
    y_pred_dollars = np.expm1(y_pred_log)
    y_test_dollars = np.expm1(y_test)

    r2 = r2_score(y_test_dollars, y_pred_dollars)
    mae = mean_absolute_error(y_test_dollars, y_pred_dollars)

    print(f"\nFinal R-squared (RÂ²): {r2:.4f}")
    print(f"Final Mean Absolute Error (MAE): ${mae:,.2f}")

    # --- 8. Save the Final Model and Supporting Objects ---
    model_filename = 'property_price_model_final.joblib'
    print(f"\nStep 8: Saving final model and supporting objects...")
    joblib.dump(best_model, model_filename)
    joblib.dump(kmeans, 'location_cluster_model.joblib')
    joblib.dump(district_price_per_m2, 'district_price_model.joblib')
    print(f"Models saved successfully.")

if __name__ == '__main__':
    train_and_evaluate_model()
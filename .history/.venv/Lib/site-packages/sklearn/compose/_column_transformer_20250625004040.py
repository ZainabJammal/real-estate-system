import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split, KFold
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
import joblib
import warnings

# --- Setup ---
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
optuna.logging.set_verbosity(optuna.logging.WARNING)

def train_and_evaluate_model():
    """
    V8: Definitive professional workflow.
    - Per-type outlier handling.
    - Geospatial features using K-Means.
    - Correctly structured Optuna trial that manually handles preprocessing
      to work with early stopping, preventing the NotFittedError.
    - Final model is re-wrapped into a clean pipeline for deployment.
    """
    # --- 1. Load and Clean Data ---
    print("Step 1: Loading and Cleaning Data...")
    df = pd.read_csv('properties.csv')
    df.drop_duplicates(subset=['id'], inplace=True)
    df = df[(df['price_$'] > 1000) & (df['size_m2'] > 10)].copy()

    # --- 2. Per-Type Outlier Handling ---
    print("\nStep 2: Handling Outliers Within Each Property Type...")
    def remove_outliers_by_group(data, group_col, target_col, lower_q=0.01, upper_q=0.99):
        q_low = data.groupby(group_col)[target_col].transform(lambda x: x.quantile(lower_q))
        q_high = data.groupby(group_col)[target_col].transform(lambda x: x.quantile(upper_q))
        return data[(data[target_col] >= q_low) & (data[target_col] <= q_high)]

    original_rows = len(df)
    df = remove_outliers_by_group(df, 'type', 'price_$')
    df = remove_outliers_by_group(df, 'type', 'size_m2')
    print(f"Removed {original_rows - len(df)} rows as per-type outliers.")

    # --- 3. Feature Engineering ---
    print("\nStep 3: Feature Engineering...")
    types_with_rooms = ['Apartment', 'House/Villa', 'Chalet', 'Office', 'Residential Building']
    df['bedrooms'] = np.where((df['type'].isin(types_with_rooms)) & (df['bedrooms'] == 0), np.nan, df['bedrooms'])
    df['bathrooms'] = np.where((df['type'].isin(types_with_rooms)) & (df['bathrooms'] == 0), np.nan, df['bathrooms'])
    for col in ['bedrooms', 'bathrooms']:
        df[col].fillna(df.groupby('type')[col].transform('median'), inplace=True)
        df[col].fillna(1, inplace=True)
        df[col] = df[col].astype(int)

    district_price_per_m2 = df.groupby('district')['price_$'].sum() / df.groupby('district')['size_m2'].sum()
    df['district_price_per_m2'] = df['district'].map(district_price_per_m2)
    df['district_price_per_m2'].fillna(df['district_price_per_m2'].median(), inplace=True)

    print("Creating geospatial features...")
    kmeans = KMeans(n_clusters=15, random_state=42, n_init='auto')
    df['location_cluster'] = kmeans.fit_predict(df[['latitude', 'longitude']]).astype(str)

    # --- 4. Data Splitting & Preprocessing Definition ---
    print("\nStep 4: Defining Preprocessing and Splitting Data...")
    X = df.drop(columns=['id', 'city', 'created_at', 'price_$'])
    y = np.log1p(df['price_$'])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    numerical_features = ['size_m2', 'latitude', 'longitude', 'district_price_per_m2']
    count_features = ['bedrooms', 'bathrooms']
    categorical_features = ['district', 'province', 'type', 'location_cluster']

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('counts', StandardScaler(), count_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='drop'
    )

    # --- 5. Optuna Objective Function (Corrected Workflow) ---
    print("\nStep 5: Defining Optuna Objective...")
    def objective(trial):
        params = {
            'objective': 'regression_l1', 'metric': 'mae', 'random_state': 42, 'n_jobs': -1,
            'n_estimators': 2000,
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),
            'num_leaves': trial.suggest_int('num_leaves', 20, 60),
            'max_depth': trial.suggest_int('max_depth', 5, 8),
            'min_child_samples': trial.suggest_int('min_child_samples', 20, 50),
            'subsample': trial.suggest_float('subsample', 0.7, 0.9),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9)
        }
        
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        scores = []
        for train_index, val_index in kf.split(X_train):
            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]
            
            # *** WORKFLOW FIX: Manually fit preprocessor on fold, then transform ***
            # 1. Fit the preprocessor on the training part of the fold
            preprocessor_fold = preprocessor.fit(X_train_fold)
            
            # 2. Transform both training and validation parts
            X_train_fold_processed = preprocessor_fold.transform(X_train_fold)
            X_val_fold_processed = preprocessor_fold.transform(X_val_fold)

            # 3. Fit the model with early stopping on the processed data
            model = lgb.LGBMRegressor(**params)
            model.fit(X_train_fold_processed, y_train_fold, 
                      eval_set=[(X_val_fold_processed, y_val_fold)],
                      eval_metric='mae',
                      callbacks=[lgb.early_stopping(50, verbose=False)])
            
            preds = model.predict(X_val_fold_processed)
            scores.append(mean_absolute_error(y_val_fold, preds))

        return np.mean(scores)

    # --- 6. Run Optuna Study ---
    print("\nStep 6: Running Optuna Study...")
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=70) # 70 trials is a thorough search

    print("\nBest parameters found: ", study.best_params)
    
    # --- 7. Train Final Model ---
    print("\nStep 7: Training Final Model with Best Parameters...")
    
    # Create the final, clean pipeline for deployment
    final_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', lgb.LGBMRegressor(objective='regression_l1', random_state=42, **study.best_params))
    ])
    
    # Fit the entire pipeline on the full training data
    final_pipeline.fit(X_train, y_train)
    print("Final model training complete.")

    # --- 8. Final Evaluation ---
    print("\n--- Step 8: Final Model Evaluation on Test Set ---")
    y_pred_log = final_pipeline.predict(X_test)
    y_pred_dollars = np.expm1(y_pred_log)
    y_test_dollars = np.expm1(y_test)

    r2 = r2_score(y_test_dollars, y_pred_dollars)
    mae = mean_absolute_error(y_test_dollars, y_pred_dollars)

    print(f"\nFinal R-squared (RÂ²): {r2:.4f}")
    print(f"Final Mean Absolute Error (MAE): ${mae:,.2f}")

    # --- 9. Save the Final Model and Supporting Objects ---
    model_filename = 'property_price_model_final.joblib'
    print(f"\nStep 9: Saving final model and supporting objects...")
    joblib.dump(final_pipeline, model_filename)
    joblib.dump(kmeans, 'location_cluster_model.joblib')
    joblib.dump(district_price_per_m2, 'district_price_model.joblib')
    print(f"Models saved successfully to {model_filename}, location_cluster_model.joblib, and district_price_model.joblib")

if __name__ == '__main__':
    train_and_evaluate_model()